{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35ce611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       timestamp  visitorid event  itemid  transactionid\n",
      "0  1433224214164     992329  view  248676            NaN\n",
      "1  1433223203944     125625  view   17655            NaN\n",
      "2  1433222147345    1076270  view  262799            NaN\n",
      "3  1433221377547    1153198  view  388242            NaN\n",
      "4  1433223176926     629333  view  128394            NaN\n",
      "Interaction matrix: (57734, 42725)\n",
      "Collaborative Filtering model trained and saved.\n"
     ]
    }
   ],
   "source": [
    "# Retailrocket Model Training\n",
    "\n",
    "# 1. Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.sparse import csr_matrix\n",
    "import faiss\n",
    "\n",
    "# Make sure the models directory exists\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# 2. Load Filtered Events\n",
    "events = pd.read_csv('../data/filtered_events.csv')\n",
    "print(events.head())\n",
    "\n",
    "# 3. Prepare Interaction Matrix (for Collaborative Filtering)\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "events['user_id_enc'] = user_encoder.fit_transform(events['visitorid'])\n",
    "events['item_id_enc'] = item_encoder.fit_transform(events['itemid'])\n",
    "\n",
    "n_users = events['user_id_enc'].nunique()\n",
    "n_items = events['item_id_enc'].nunique()\n",
    "\n",
    "interaction_matrix = csr_matrix(\n",
    "    (np.ones(events.shape[0]), (events['user_id_enc'], events['item_id_enc'])),\n",
    "    shape=(n_users, n_items)\n",
    ")\n",
    "\n",
    "print(f\"Interaction matrix: {interaction_matrix.shape}\")\n",
    "\n",
    "# 4. Train Simple Collaborative Filtering (Matrix Factorization Approx.)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "user_factors = svd.fit_transform(interaction_matrix)\n",
    "item_factors = svd.components_.T\n",
    "\n",
    "# Save Encoders and Factors\n",
    "pickle.dump(user_encoder, open('../models/user_encoder.pkl', 'wb'))\n",
    "pickle.dump(item_encoder, open('../models/item_encoder.pkl', 'wb'))\n",
    "np.save('../models/user_factors.npy', user_factors)\n",
    "np.save('../models/item_factors.npy', item_factors)\n",
    "\n",
    "print(\"Collaborative Filtering model trained and saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097c6785",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 5. Content-Based Filtering (using item metadata)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m item_properties \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/item_properties_part1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Merge with part2 if needed\u001b[39;00m\n\u001b[1;32m      3\u001b[0m item_properties \u001b[38;5;241m=\u001b[39m item_properties\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Take latest properties\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# 5. Content-Based Filtering (using item metadata)\n",
    "item_properties = pd.read_csv('../data/item_properties_part1.csv')  # Merge with part2 if needed\n",
    "item_properties = item_properties.dropna()\n",
    "\n",
    "# Take latest properties\n",
    "item_properties_latest = item_properties.sort_values('timestamp').drop_duplicates('itemid', keep='last')\n",
    "\n",
    "# Example: One-hot encode categoryid\n",
    "item_metadata = item_properties_latest[['itemid', 'property', 'value']].pivot_table(\n",
    "    index='itemid', columns='property', values='value', aggfunc='first'\n",
    ")\n",
    "\n",
    "item_metadata = item_metadata.fillna('unknown')  # Fill missing\n",
    "\n",
    "# Simple text embedding using one-hot encoding\n",
    "item_metadata_encoded = pd.get_dummies(item_metadata.apply(lambda x: str(x)))\n",
    "\n",
    "# Match items with encoded ids\n",
    "item_metadata_encoded = item_metadata_encoded.reset_index()\n",
    "item_metadata_encoded['item_id_enc'] = item_encoder.transform(item_metadata_encoded['itemid'])\n",
    "\n",
    "item_feature_matrix = csr_matrix(item_metadata_encoded.drop(['itemid', 'item_id_enc'], axis=1).values)\n",
    "\n",
    "# Save FAISS index for fast similarity search\n",
    "d = item_feature_matrix.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(item_feature_matrix.toarray())\n",
    "\n",
    "faiss.write_index(index, \"../models/item_content_index.faiss\")\n",
    "print(\"Content-Based model (FAISS index) trained and saved.\")\n",
    "\n",
    "# 6. Hybrid Recommender (Weighted Fusion)\n",
    "def hybrid_score(user_vector, item_vector_cf, item_vector_content, alpha=0.5):\n",
    "    \"\"\"Simple weighted hybrid score between CF and Content-Based\"\"\"\n",
    "    return alpha * np.dot(user_vector, item_vector_cf) + (1 - alpha) * np.dot(user_vector, item_vector_content)\n",
    "\n",
    "# 7. Prepare Data for Learning-to-Rank (LTR)\n",
    "# (Synthetic Example: Click = relevant, View = less relevant)\n",
    "events['event_weight'] = events['event'].map({\n",
    "    'view': 1,\n",
    "    'addtocart': 2,\n",
    "    'transaction': 3\n",
    "}).fillna(0)\n",
    "\n",
    "X = events[['user_id_enc', 'item_id_enc']]\n",
    "y = events['event_weight']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# LightGBM expects group info (how many items per user in train set)\n",
    "train_group = X_train.groupby('user_id_enc').size().values\n",
    "val_group = X_val.groupby('user_id_enc').size().values\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train, group=train_group)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, group=val_group, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "ltr_model = lgb.train(params, lgb_train, valid_sets=[lgb_val], num_boost_round=100, early_stopping_rounds=10)\n",
    "\n",
    "# Save LTR model\n",
    "ltr_model.save_model('../models/ltr_model.txt')\n",
    "\n",
    "print(\"LTR model trained and saved.\")\n",
    "\n",
    "# 8. Quick Evaluation\n",
    "# Predict scores\n",
    "y_pred_val = ltr_model.predict(X_val)\n",
    "\n",
    "# Group by user_id\n",
    "grouped = X_val.copy()\n",
    "grouped['y_true'] = y_val\n",
    "grouped['y_pred'] = y_pred_val\n",
    "\n",
    "ndcg_per_user = []\n",
    "for user_id, group in grouped.groupby('user_id_enc'):\n",
    "    if group.shape[0] > 1:\n",
    "        ndcg = ndcg_score([group['y_true'].values], [group['y_pred'].values])\n",
    "        ndcg_per_user.append(ndcg)\n",
    "\n",
    "mean_ndcg = np.mean(ndcg_per_user)\n",
    "print(f\"Validation NDCG: {mean_ndcg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
